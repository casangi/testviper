INFO     viperlog:parameter.py:227 Module path: [38;2;50;50;205m/home/runner/work/testviper/testviper/src/toolviper[0m
WARNING  viperlog:logger.py:109 It is recommended that the local cache directory be set using the [38;2;50;50;205mdask_local_dir[0m parameter.
INFO     distributed.http.proxy:proxy.py:85 To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
INFO     distributed.scheduler:scheduler.py:1766 State start
INFO     distributed.scheduler:scheduler.py:4282   Scheduler at:     tcp://127.0.0.1:39195
INFO     distributed.scheduler:scheduler.py:4297   dashboard at:  http://127.0.0.1:8787/status
INFO     distributed.scheduler:scheduler.py:8177 Registering Worker plugin shuffle
INFO     distributed.nanny:nanny.py:368         Start Nanny at: 'tcp://127.0.0.1:32965'
INFO     distributed.nanny:nanny.py:368         Start Nanny at: 'tcp://127.0.0.1:34397'
INFO     distributed.scheduler:scheduler.py:4635 Register worker addr: tcp://127.0.0.1:34571 name: 1
INFO     distributed.scheduler:scheduler.py:6224 Starting worker compute stream, tcp://127.0.0.1:34571
INFO     distributed.core:core.py:883 Starting established connection to tcp://127.0.0.1:51288
INFO     distributed.scheduler:scheduler.py:4635 Register worker addr: tcp://127.0.0.1:43529 name: 0
INFO     distributed.scheduler:scheduler.py:6224 Starting worker compute stream, tcp://127.0.0.1:43529
INFO     distributed.core:core.py:883 Starting established connection to tcp://127.0.0.1:51292
INFO     distributed.scheduler:scheduler.py:1766 State start
INFO     distributed.scheduler:scheduler.py:4282   Scheduler at:     tcp://127.0.0.1:42375
INFO     distributed.scheduler:scheduler.py:4297   dashboard at:  http://127.0.0.1:36847/status
INFO     distributed.scheduler:scheduler.py:8177 Registering Worker plugin shuffle
INFO     distributed.nanny:nanny.py:368         Start Nanny at: 'tcp://127.0.0.1:34717'
INFO     distributed.nanny:nanny.py:368         Start Nanny at: 'tcp://127.0.0.1:35665'
INFO     distributed.nanny:nanny.py:368         Start Nanny at: 'tcp://127.0.0.1:35073'
INFO     distributed.nanny:nanny.py:368         Start Nanny at: 'tcp://127.0.0.1:40079'
INFO     distributed.scheduler:scheduler.py:4635 Register worker addr: tcp://127.0.0.1:43735 name: 3
INFO     distributed.scheduler:scheduler.py:6224 Starting worker compute stream, tcp://127.0.0.1:43735
INFO     distributed.core:core.py:883 Starting established connection to tcp://127.0.0.1:46330
INFO     distributed.scheduler:scheduler.py:4635 Register worker addr: tcp://127.0.0.1:35441 name: 2
INFO     distributed.scheduler:scheduler.py:6224 Starting worker compute stream, tcp://127.0.0.1:35441
INFO     distributed.core:core.py:883 Starting established connection to tcp://127.0.0.1:46340
INFO     distributed.scheduler:scheduler.py:4635 Register worker addr: tcp://127.0.0.1:46071 name: 1
INFO     distributed.scheduler:scheduler.py:6224 Starting worker compute stream, tcp://127.0.0.1:46071
INFO     distributed.core:core.py:883 Starting established connection to tcp://127.0.0.1:46348
INFO     distributed.scheduler:scheduler.py:4635 Register worker addr: tcp://127.0.0.1:37113 name: 0
INFO     distributed.scheduler:scheduler.py:6224 Starting worker compute stream, tcp://127.0.0.1:37113
INFO     distributed.core:core.py:883 Starting established connection to tcp://127.0.0.1:46362
INFO     distributed.scheduler:scheduler.py:5959 Receive client connection: MenrvaClient-397b0659-5650-11f0-8ad4-6045bd7f8793
INFO     distributed.core:core.py:883 Starting established connection to tcp://127.0.0.1:46378
INFO     distributed.scheduler:scheduler.py:8177 Registering Worker plugin worker_logger
INFO     viperlog:logger.py:55 Client <MenrvaClient: 'tcp://127.0.0.1:42375' processes=4 threads=4, memory=15.62 GiB>
WARNING  viperlog:logger.py:109 File exists: [38;2;50;50;205m/home/runner/work/testviper/testviper/src/toolviper/src/toolviper/utils/data/.dropbox[0m
INFO     viperlog:logger.py:55 Updating file metadata information ... 
INFO     viperlog:logger.py:55 Creating path:[38;2;50;50;205m/tmp/test[0m
INFO     viperlog:logger.py:55 Partition scheme that will be used: ['DATA_DESC_ID', 'OBS_MODE', 'OBSERVATION_ID']
INFO     viperlog:logger.py:55 Partition scheme that will be used: ['DATA_DESC_ID', 'OBS_MODE', 'OBSERVATION_ID']
INFO     viperlog:logger.py:55 Number of partitions: 4
INFO     viperlog:logger.py:55 OBSERVATION_ID [0], DDI [0], STATE [32 33 34 23 24 25 30 31 37], FIELD [0 1 2], SCAN [ 9 17 21 25]
INFO     viperlog:logger.py:55 OBSERVATION_ID [1], DDI [0], STATE [32 33 34 23 24 25 30 31 37], FIELD [0 1 2], SCAN [26 34 38 42]
INFO     viperlog:logger.py:55 OBSERVATION_ID [2], DDI [0], STATE [32 33 34], FIELD [0 1 2], SCAN [43]
INFO     viperlog:logger.py:55 OBSERVATION_ID [3], DDI [0], STATE [48 49 50 39 40 41 46 47 53], FIELD [0 1 2], SCAN [48 56 60 64]
WARNING  viperlog:logger.py:109 File exists: [38;2;50;50;205m/home/runner/work/testviper/testviper/src/toolviper/src/toolviper/utils/data/.dropbox[0m
INFO     viperlog:logger.py:55 Updating file metadata information ... 
INFO     viperlog:logger.py:55 File exists: /tmp/test/Antennae_North.cal.lsrk.split.ms
INFO     viperlog:logger.py:55 Partition scheme that will be used: ['DATA_DESC_ID', 'OBS_MODE', 'OBSERVATION_ID', 'FIELD_ID']
INFO     viperlog:logger.py:55 Partition scheme that will be used: ['DATA_DESC_ID', 'OBS_MODE', 'OBSERVATION_ID', 'FIELD_ID']
INFO     viperlog:logger.py:55 Number of partitions: 12
INFO     viperlog:logger.py:55 OBSERVATION_ID [0], DDI [0], STATE [32 23 30 37], FIELD [0], SCAN [ 9 17 21 25]
INFO     viperlog:logger.py:55 OBSERVATION_ID [0], DDI [0], STATE [33 24 31], FIELD [1], SCAN [ 9 17 21]
INFO     viperlog:logger.py:55 OBSERVATION_ID [0], DDI [0], STATE [34 25 32], FIELD [2], SCAN [ 9 17 21]
INFO     viperlog:logger.py:55 OBSERVATION_ID [1], DDI [0], STATE [32 23 30 37], FIELD [0], SCAN [26 34 38 42]
INFO     viperlog:logger.py:55 OBSERVATION_ID [1], DDI [0], STATE [33 24 31], FIELD [1], SCAN [26 34 38]
INFO     viperlog:logger.py:55 OBSERVATION_ID [1], DDI [0], STATE [34 25 32], FIELD [2], SCAN [26 34 38]
INFO     viperlog:logger.py:55 OBSERVATION_ID [2], DDI [0], STATE [32], FIELD [0], SCAN [43]
INFO     viperlog:logger.py:55 OBSERVATION_ID [2], DDI [0], STATE [33], FIELD [1], SCAN [43]
INFO     viperlog:logger.py:55 OBSERVATION_ID [2], DDI [0], STATE [34], FIELD [2], SCAN [43]
INFO     viperlog:logger.py:55 OBSERVATION_ID [3], DDI [0], STATE [48 39 46 53], FIELD [0], SCAN [48 56 60 64]
INFO     viperlog:logger.py:55 OBSERVATION_ID [3], DDI [0], STATE [49 40 47], FIELD [1], SCAN [48 56 60]
INFO     viperlog:logger.py:55 OBSERVATION_ID [3], DDI [0], STATE [50 41 48], FIELD [2], SCAN [48 56 60]